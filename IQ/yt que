
Q12. What is git init, and why do we use it?

Answer:
"git init is used to create a new Git repository in a project. 
It makes a hidden .git folder which stores all commit history and configuration. 
We use it when we want Git to start tracking changes in our project."

Q13. What is the difference between git pull and git fetch?

Answer:
"git fetch only downloads the latest changes from the remote but does not merge them into my local branch. 
git pull does both — it fetches the changes and merges them directly into my branch. 
I use fetch when I want to review changes first, 
and pull when I want to update my branch immediately."


----------------------------------
1.how to manage certification 
2.how to integrate sonar qube
3.diffence between add and copy in docker 
4.providers
5.how to increase capacity of EC2 if is running and full
6.how to communicate 2 vpc having same cidr
7.if EC2 is created manually and it is not reflecting in tf
8.if deployment is ok in stage and dev but in prod it is pending 
9.which ingress controller u r using for kubernetes 
10.how to get cpu utilisation of pod
11.how to get pods depending on environment 
12. what is waf
13. If you need to do access of your application to 200 users how to do it
14. Count and for each
15. What ingress you r using 
16. How the flow of your application using cluster 
17.route 53 workflow

18.how to manage different state file for different environment in tf

We use a single Remote Backend (S3 + DynamoDB) for all environments. 
For each environment, we maintain a separate state file (key path).
 Even if 5 users are working simultaneously, state locking (via DynamoDB) 
ensures that two users cannot make changes to the same environment at the
 same time. This prevents conflicts and keeps the state securely shared."


19.if tfstate file is deleted how to do then
20.if cpu utilisation is more how to debug
-------------------------------------------------------------------------
21.port mapping command in kubernetes

kubectl port-forward pod/<pod-name> 8080:80 -n <namespace>
kubectl port-forward svc/<service-name> 8080:80 -n <namespace>
-------------------------------------------------------

22.Sonarqube set up
23.Can you describe your experience with CI/CD pipelines, particularly using Jenkins and Terraform?
24.2/3 members are working at same time how to restrict to use module
25.S3 bucket terraform module
26.Egress service 
27. Provisonal in tf
28.Work on K8s
---------------------------------------------------------------------

29.Vpc already created on AWS console how to get vpc id using tf=

Using Data Source (Recommended)

Terraform AWS provider gives a aws_vpc data source.

Example: Fetch existing VPC by tag

data "aws_vpc" "existing" {
  filter {
    name   = "tag:Name"
    values = ["my-existing-vpc"]
  }
}

1. Can you explain your project workflow?
2. What branching strategy you follow?
3. write Terraform code for s3 bucket?
---------------------------------------------------------------------
4. Have you handled any CVE in you last project and how you handled it?
Docker base image

Old Alpine image had issue → I used the latest Alpine → rebuilt image. ✅

4. Kubernetes CVE

Cluster had a bug → I upgraded EKS version → services worked fine. ✅

5. Nginx ingress

Ingress image was vulnerable → I pulled latest version → redeployed and fixed. ✅
----------------------------------------------------------------------------------
5. Explain me the challenge you had in you last project and how did you resolved it.
6. I showed by experience in Terraform and Jenkins. On which they asked 
if you were asked to work on other tools like AWS CDK or argo cd? 
how do you handle that, what would be approach?
--------------------------------------------------------------------------------
7. What is cloud formation linter?

CloudFormation Linter = A static analysis tool to validate AWS 
CloudFormation templates for correctness and best practices before deployment.

---------------------------------------------------------------------------------
8. What is Dockerfile?
9. Tell me which are the orchestration tool you have used?
10. Have you worked on hem chat and how do you write yaml manifest file?
11. What are k8s deployment strategies?
12. Have you worked in agile environment and what tool you used to tracking?
13. Have you worked on confluence?
14. What is docker container and docker daemon?
15.  Do you have knowledge about gitOps?





-------------------------------------------------------------------------------------------------------


3) How can you give temporary access of 5 mins to an object stored in S3 bucket to an external user while keeping bucket private ?

“If my S3 bucket is private but I need to give temporary access to an external user,
I would generate a pre-signed URL. This URL is created using AWS CLI or SDK and allows 
access to a specific object for a limited time— for example, 5 minutes. The URL automatically
expires after that duration, ensuring the bucket remains private and secure.”
aws s3 presign s3://bucket-name/object --expires-in 300
----------------------------------------------------------------------------------------------------------------------
5) I have two AWS S3 bucket in 2 different regions.In one bucket static website is hosted and in 2nd images are stored. 
The static website has to fetch images. What kind of setup will you do ?

Bucket A (Region 1): Static website (HTML, CSS, JS).
Bucket B (Region 2): Images.
Requirement → Website must fetch images from the other bucket, securely and efficiently.


Enable Cross-Origin Resource Sharing (CORS):

Since your static site and image bucket are in different regions, browser requests need CORS rules on the image bucket.


Example CORS policy on Bucket B (images):

<CORSConfiguration>
  <CORSRule>
    <AllowedOrigin>http://my-website.s3-website-us-east-1.amazonaws.com</AllowedOrigin>
    <AllowedMethod>GET</AllowedMethod>
    <AllowedHeader>*</AllowedHeader>
  </CORSRule>
</CORSConfiguration>

Make image bucket objects accessible:
Keep Bucket B private but allow access via:
Pre-signed URLs (if limited users).
Or Bucket Policy with CloudFront OAI (Origin Access Identity) for secure, controlled public access.


Use CloudFront for global performance:

Deploy a CloudFront distribution in front of both buckets.
One origin = Static Website bucket.
Second origin = Image bucket.
CloudFront will cache images at edge locations → better latency and cost savings.
Can also set up Origin Groups to combine origins.


Since the website is in one S3 bucket and images are in another bucket in a different region, 
I would configure CORS on the image bucket so the static website can fetch them. To improve
performance and provide secure global access, I would place CloudFront in front of both buckets. CloudFront will act as a
single distribution with multiple origins — one for the static website and one for the images — 
ensuring low latency and secure access.”
------------------------------------------------------------------------------------------------------------------------------
**“To scrape data from running nodes, I would deploy the monitoring agent as a DaemonSet if it’s a Kubernetes environment,
since DaemonSets ensure that one agent runs on every node automatically.

-------------------------------------------------------------------------
403 Forbidden
Meaning → “You are authenticated, but you don’t have permission.

401 Unauthorized
Meaning → “You are not authenticated.”
-----------------------------------------------------------------------------------


















-=-------------------------------------------------------------------------------------------------------------------
network load balancer - inegrasion section - vpc endpoint (private link)

create vpc private and public subnet - then crte ec2 in private - TG - then create NLB - network load balancer - inegrasion section - vpc endpoint (private link)
we need to select nlb in vpc private link endpoint - 
vpc endpoint crate kra on primises - in that select sevice categary other endpoint sevices - copy other endpoint name and pest heare 
ec2 - vpc endpoint (A-VPC) -  VPC ENDPOINT (b vpc) -nlb - tg - ec2
in vpc endpoint A - in that endpoint connection - accept it from b



----------------------------------------------------------------------------------------
/var/www/html/index.html
What happens when you do SSHTo your server ?
SSH connects my client to the server on port 22, authenticates with key/password, 
establishes encryption, and then gives me a secure shell to run commands.
-----------------------------------------------------------------------------------

1) Your application on EC2 experiences traffic spikes only during business hours. How would you optimize cost and performance?

“I will use Auto Scaling for EC2 so that more instances run only in business hours 
and scale down after hours. Also, I can use a Load Balancer for even traffic distribution.
For more cost saving, I can use Spot Instances or Scheduled Scaling.”
----------------------------------------------------------------------------------------
2) Your EC2 instance crashed unexpectedly. What steps would
you take to identify the issue and restore service quickly?
If my EC2 instance crashes, first I would check system logs and CloudWatch metrics to find the cause (like CPU, memory, or disk).
Next, I’d try to quickly restore service by restarting the instance or replacing it with a new one using an AMI or Auto Scaling.
After recovery, I’d analyze root cause, such as application errors or OS issues, and put preventive measures in place like monitoring and alerts.”
-----------------------------------------------------------------------------------------------------------------------------------------
3) You have deployed your app on EC2 instance. The app has to Fetch some images
from internet but it's not able to fetch. What might be the possible issues?

If my app on EC2 can’t fetch images from the internet, I would first check if the instance has internet access. 
If it’s in a private subnet, I’d verify NAT Gateway. Then, I’d check security group outbound rules for ports 
80/443 and confirm that NACLs allow return traffic.
I’d also test DNS resolution and ensure no local firewall is blocking traffic. These steps usually help 
identify and fix the issue quickly.
----------------------------------------------------------------------------------------------
4) You accidentally deleted data from S3. How do you recover it?

If I accidentally delete data from S3, the first thing I check is whether S3 Versioning is enabled.
If it is, I can easily recover the object by restoring a previous version. If Cross-Region Replication was set up,
I can also restore it from the replicated bucket. If versioning wasn’t enabled, unfortunately, the data cannot be 
recovered, but I’d analyze the CloudTrail logs
to identify the cause and immediately enable Versioning and MFA Delete to prevent such incidents in the future."
---------------------------------------------------------------------------------------------------------
6) I have 2 EC2 Instance in 2 VPC one in each. First one is running
frontend app and second is running backend. These 2 app needs to
communicate but they're not able to communicate. How will you fix this ?

“Since the two EC2s are in different VPCs, they can’t communicate directly.
I would enable communication either by creating a VPC peering connection or using a Transit Gateway. 
Then, I’d update the route tables and security groups to allow traffic between the frontend and backend subnets.”
----------------------------------------------------------------------------------------------------------

7) I have an app running on EC2. Product icons are stored in S3 bucket which
Needs to be shown on web home page. When my application is trying to fetch
it's resulting into error. What might be the problem here?

I will check bucket policy and ensure objects are accessible either via pre-signed URLs or correct IAM role. If the frontend directly
loads icons, I’ll configure proper CORS policy on the S3 bucket. Also, if private access is required, I’ll set up an S3 VPC endpoint.”
-----------------------------------------------------------------------------------------------------------------------------------------

10) how to distribute 10 pods equally over 10 nodes in Kubernetes?
Like for log collecting metric beat, dynatrace agent etc

To distribute one pod per node, I’ll use a DaemonSet. It ensures that exactly one pod of my app runs on each node, 
which is ideal for log collectors or monitoring agents like metricbeat and dynatrace."

---------------------------------------------------------------------------------------------------------------------

13) I have created a VPC manually. Now I have to create one EC2 instance
using Terraform. How will you pass VPC id here in Terraform script?

If the VPC is created manually, Terraform doesn’t track it, so I would pass the VPC ID as a variable in my script, 
or use a data block to fetch the existing VPC by its tags. This way Terraform can still provision the EC2 instance inside that VPC.”

data "aws_vpc" "my_vpc" {
  filter {
    name   = "tag:Name"
    values = ["my-manual-vpc"]
  }
}

resource "aws_instance" "my_ec2" {
  ami           = "ami-1234567890abcdef0"
  instance_type = "t2.micro"
  subnet_id     = "subnet-xxxxxx"
  vpc_security_group_ids = [aws_security_group.my_sg.id]
  tags = {
    Name = "MyEC2"
  }
}
-----------------------------------------------------------------------------------------------------
14) You have provisioned a VPC using Terraform and someone deleted
it manually. Now if you run Terraform apply, how will your pipeline behave?

If someone deletes the VPC manually, Terraform will detect this as a drift in the infrastructure state during the plan phase.
Since the VPC is missing, Terraform will show that the resource needs to be created again.
When I run terraform apply, it will re-create the VPC according to the configuration defined in my .tf files." ✅
-----------------------------------------------------------------------------------------------------------------------

15) I have 2 modules written one for VPC and second for EC2. EC2 modules need VPC id
to provision. How will you ensure that EC2 module get VPC id at runtime?

I will use module outputs to export the VPC ID from the VPC module and pass it as an input variable to the EC2 module. 
Terraform automatically handles dependencies and ensures the correct order of resource creation.
----------------------------------------------------------------------------------------------------------------
Where are system logs stored?

Most logs are in /var/log/ directory.

Some important files:

/var/log/syslog → General system logs (Ubuntu/Debian).

/var/log/messages → General system logs (RHEL/CentOS).

/var/log/auth.log → Authentication & SSH login logs.

/var/log/dmesg → Kernel & boot logs.

/var/log/nginx/ or /var/log/apache2/ → Web server logs.
------------------------------------------------------------------------------------------------------
17) I am trying to run one script file on my Linux server and
getting permission error. How will you troubleshoot this ?

If I get a permission error while running a script, first I’ll check if the file has execute permission 
(chmod +x). Then I’ll verify ownership and whether I’m running it correctly with ./script.sh or bash script.sh. 
If it’s copied from Windows,
I’ll fix line endings using dos2unix. Finally, if SELinux is enabled, I’ll check for security restrictions.
----------------------------------------------------------------------------------------------------------------

18) Linux server is experiencing slow performance.
How will you debug this slowness?

For Linux server slowness, I will:

Check CPU, memory, and load using top, htop, or uptime.

Verify disk I/O and space with iostat, df -h.

Review logs (/var/log/) and network usage with netstat/ss
----------------------------------------------------------------------------

I have to deploy an application. In which scenario you will choose EC2 
and Lambda ?I have to deploy an application. In which scenario

If my application needs to run all the time, has complex setup, or requires full control over 
the server, I will choose EC2.But if my application has small tasks, runs only when triggered, and I want 
to save cost without managing servers, I will choose Lambda.”
---------------------------------------------------------------------------------------
How will you troubleshoot 403 error code?-----

A 403 error means forbidden – the server understood the request but refused to allow it.
To troubleshoot, first I will check permissions. For example: in web servers I will check 
file/folder permissions, in S3 I will check bucket policy, in APIs I will check authentication 
and IAM roles. Then I verify if the user or service account has the correct access rights.
Finally, I review logs (web server logs, CloudTrail, or app logs) to see the exact reason.”
-------------------------------------------------------------------------------------

“I manage my infrastructure using Infrastructure as Code with Terraform,
which keeps everything automated and version-controlled. Deployments are integrated through CI/CD pipelines in Jenkins, and I 
use Prometheus and Grafana for monitoring along with ELK stack for logging.
This ensures scalability, consistency, and easy troubleshooting.”

----------------------------------------------------------------------------------------------------------------------------------
What will happen if your infrastructure has been deleted manually
and
you run infra provisioning pipeline again ?

If infra is deleted manually, the pipeline will recreate it because Terraform compares actual state vs. 
desired state in code. As long as the state file is intact, 
infra will be provisioned back. But if the state file is also lost, Terraform may try to create duplicate resources.”
-------------------------------------------------------------------------------------------------------------------------------------

How will you push code from local to GitHub repository
You can answer in simple steps like this in interview:

First, initialize git in project → git init.

Add remote GitHub repo → git remote add origin <repo-URL>.

Stage and commit code → git add . && git commit -m "Initial commit".

Push to GitHub → git push origin main (or master). ✅
--------------------------------------------------------------------------------------------------------------------------
4) Suppose you have an EC2 Machine and you performed some task today.
Now you have to perform the same task tomorrow and so on
but on a new EC2 Machine. What will be the best way to do task here?

"I will never do the same task manually again and again. I’ll automate it. Depending on the requirement:

For pre-baked configuration → I’ll use an AMI.

For startup scripts → I’ll use User Data.

For large-scale infra management → I’ll use Terraform with Ansible so that every time a new EC2 is launched, tasks are performed automatically."
----------------------------------------------------------------------------------------------------------------------------
6) In S3 bucket, I want to keep my bucket private and at the same time
I want my one of object can be fetched by one user for specific 15 mins.
How to achieve this ?

"I will keep my bucket private and use an S3 Pre-Signed URL to share the object. 
The Pre-Signed URL can be generated through AWS CLI or SDK, and I will set the expiry to 15 minutes.
After that time, the link won’t work anymore. This ensures the bucket is private, but still allows controlled access when needed."
---------------------------------------------------------------------------------------------------------------------------------------

18) Which AWS services can be used to trigger notification whenever
your database storage gets consumed more than 80%?

"I’ll configure a CloudWatch alarm on the database’s FreeStorageSpace metric. Once utilization crosses 80%, the alarm will trigger 
an SNS topic, which will send me a notification (email/SMS). This ensures proactive alerting before the database runs out of storage."
--------------------------------------------------------------------------------------------------------






