managed and self-hosted cluster 
managed - controlled plain cloud and data plan by ourselves 
self-hosted - both managed by ourself 
--------------------------------------------------------------------

Advanced Kubernetes Scheduling
This repository demonstrates advanced scheduling techniques in Kubernetes, including Node Selector, Node Affinity, Taints & Tolerations,
and Pod Affinity & Anti-Affinity. These techniques allow you to fine-tune how your workloads are distributed across your
cluster, ensuring optimal performance and resource utilization.

Topics Covered
1. Node Selector
Node Selector is a simple way to ensure that your pods run on specific nodes by assigning labels. This is useful when you have a node 
s with special hardware or software configurations and you want certain workloads to run only on those nodes.

Example Use Case: Targeting a node with high-performance CPUs for compute-intensive tasks.
# Label a node with a custom label
kubectl label node <node-name> high-perf-cpu=yes

# Deploy a pod targeting the node with the specified label
kubectl apply -f <your-deployment-file>.yaml

# Verify that the pod is running on the correct node
kubectl get pods -o wide --no-headers | awk -F" " '{print $1, $8}'

2. Node Affinity
Node Affinity allows more complex scheduling decisions based on node labels. Unlike Node Selector, Node Affinity supports 
multiple label expressions and can differentiate between preferred and required conditions.

Example Use Case: Ensuring that pods are deployed only on nodes with specific environmental settings or hardware.
# Label nodes with different environment labels
kubectl label node <node1-id> env=one
kubectl label node <node2-id> env=two
kubectl label node <node3-id> env=three

# Deploy a workload with Node Affinity rules
kubectl apply -f <your-deployment-file>.yaml

# Scale the deployment to observe how pods are distributed

kubectl scale deployment <deployment-name> --replicas=8


3. Taints & Tolerations

Taints are applied to nodes to prevent pods that don't tolerate the taint from being scheduled on them. 
Tolerations are applied to pods to allow them to be scheduled on tainted nodes. This feature is essential for keeping certain workloads 
separate or ensuring critical workloads are not disrupted.

Example Use Case: Ensuring that only specific pods can run on a node reserved for special tasks.
# Taint nodes to control scheduling
kubectl taint node <node-id> high-cpu=yes: NoSchedule 
kubectl taint node <node-id> med-cpu=yes:NoExecute

# Describe the node to see its taints
kubectl describe node <node-id> | grep -i high

# Deploy a pod to see if it gets scheduled based on tolerations
kubectl apply -f <your-deployment-file>.yaml

# Remove taints from nodes if needed
kubectl taint node <node-id> high-cpu-
kubectl taint node <node-id> medium- 


4. Pod Affinity & Anti-Affinity
Pod Affinity allows you to schedule pods together on the same node, while Pod Anti-Affinity ensures that pods are placed on separate nodes. 
This is useful for reducing latency between pods or ensuring high availability by spreading pods across nodes.

Example Use Case: Grouping frontend and backend pods together to reduce network latency or ensuring that replicas of the same 
service are spread out to avoid single points of failure.
# Use pod affinity or anti-affinity in your deployment YAML
kubectl apply -f <your-deployment-file>.yaml

# Scale the deployment and observe the behavior
kubectl scale deployment <deployment-name> --replicas=2

# Drain a node to see how pods are rescheduled
kubectl drain <node-id>
kubectl un cordon <node-id> 
Commands Overview
Here’s a summary of the key commands used for the topics covered:

# Label a node
kubectl label node <node-name> high-perf-cpu=yes

# Apply a deployment file
kubectl apply -f <your-deployment-file>.yaml

# Verify pod placement
kubectl get pods -o wide --no-headers | awk -F" " '{print $1, $8}'

# Label nodes for Node Affinity
kubectl label node <node1-id> env=one
kubectl label node <node2-id> env=two
kubectl label node <node3-id> env=three

# Scale a deployment
kubectl scale deployment <deployment-name> --replicas=8

# Taint nodes
kubectl taint node <node-id> high-cpu=yes:NoSchedule
kubectl taint node <node-id> med-cpu=yes:NoExecute

# Remove taints from nodes
kubectl taint node <node-id> high-cpu-
kubectl taint node <node-id> medium- 

# Drain and un cordon a node 
kubectl drain <node-id>
kubectlun cordonn <node-id> 
Conclusion
This repository provides practical examples and commands to help you understand and implement advanced scheduling techniques in Kubernetes. 
By mastering these concepts, you can ensure that your applications run efficiently and reliably in a Kubernetes environment.





----------------------------------------------------------------------------------------------------------------------------------------------------------------
node selector and node affinity diff 
In the node selector, we can pass a single expression  here
but in node affinity, we can pass multiple match expressions heare          -------vamp 
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------
pod affinity and an Pod antiaffinity 

user is trying to apply - first request heating to FE - with the help of ingress - FE is going to BE - BE is going DB 
it will give the info - but u will see a lot of latency here to avoid that we used pod affinity  
we will make a group of FE AND BE DB TO AVOID latency 
next time a customer tries to access it - the route will be transferred to FE-BE-DB   
--------------------------------------------------------------------------------------------------------------------------------------------------------
antipodes affinity 
means just like demonset -but not purely - it makes sure that no multiple pods on the same node |spreadingg pod across multipldnodese 

We don't want schedule the same pod on a single node?
--------------------------------------------------------------------------------------------------------------------------------------



node selector by WORKER NODE id

 Node selector: 
        kubernetes.io/hostname: i-00a968443bbbdd1cf


apiVersion: apps/v1
kind: Deployment
metadata:
Labels: 
    app: myapp01
  name: myapp01
Spec: 
  replicas: 3
  Selector: 
    matchLabels:
      app: myapp01
  Template: 
    Metadata: 
      Labels: 
        app: myapp01
    Spec: 
      Containers: 
      - image: kiran2361993/kubegame:v2
        Name: cube game 
     node selector: 
        kubernetes.io/hostname: i-00a968443bbbdd1cf

apply it 
ku gets podstechch node vr pod to create a hotel 
 
 ku scale deployment nyapp0l --replicas 10 
scale kelyavr pn tych node vr hotel 

delete all
----------------------------------------------------------------------------------


node selector by label

kubectl label node i-08cb4fadSc2fd467e high-perf-cpu=yes
kubectl label node i-0d255ef313a497963 low-perf-cpu=yes
kubectl label node i-0dcf19a32f70443af low-perf-cpu=yes


 Node selector: 
        high-perf-cpu: "yes"


first label div legal 

kubectl label node i-08cb4fadSc2fd467e high-perf-cpu=yes
kubectl label node i-0d255ef313a497963 low-perf-cpu=yes
kubectl label node i-0dcf19a32f70443af low-perf-cpu=yes

kubectl describe node i-08cb4fad5c2fd467e | grep -i high
kubectl describe node i-0d255ef313a497963 | grep -i med
kubectl describe node i-0dcf19a32f70443af | grep -i low


apiVersion: apps/v1
kind: Deployment
metadata:
  Labels: 
    app: myapp02
  name: myapp02
Spec: 
  replicas: 4
  Selector: 
    matchLabels:
      app: myapp02
  Template: 
    Metadata: 
      Labels: 
        app: myapp02
    Spec: 
      Containers: 
      - image: kiran2361993/kubegame:v2
        name: cube game 
     node selector: 
        high-perf-cpu: "yes"

apply it
ku get pods -o wide
2 node la 1 ch samlabelle ahe 

ku scale deployment myapp02 --replicas 10
delete all
------------------------------------------------------------------------------------------------------------------
node affinity 
required preferred 
 
required 
 
kubectl label node i-08cb4fad5c2fd467e env=one
kubectl label node i-0d255ef313a497963 env=two
kubectl label node i-0dcf19a32f70443af env=three

apiVersion: apps/v1
kind: Deployment
metadata:
  Labels: 
    run: required-hard
  name: required-hard
Spec: 
  replicas: 4
  Selector: 
    matchLabels:
      run: required-hard
  Template: 
    Metadata: 
      Labels: 
        run: required-hard
    Spec: 
      Affinity: 
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - match expressions: 
              - key: env
                operator: In
                values:
                - one
                - two
      containers:
      - image: nginx: latest 
        name: required-hard

apply it
ku get pods -o wide --no-headers

hya 2 vr depoly honar 
node i-08cb4fad5c2fd467e env=one
node i-0d255ef313a497963 env=two 
------------------------------------------------------------ preferred 
 
apiVersion: apps/v1
kind: Deployment
metadata:
Labels: 
    run: preferred-soft
  name: preferred-soft
Spec: 
  replicas: 4
  Selector: 
    matchLabels:
      run: preferred-soft
  Template: 
    Metadata: 
      Labels: 
        run: preferred-soft
    Spec: 
      Affinity: 
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 40
            preference:
              Match expressions: 
              - key: env
                operator: In
                values:
                - one
          - weight: 30
            preference:
             Match expressions: 
              - key: env
                operator: In
                values:
                - two
          - weight: 30
            preference:
            Match expressions: 
              - key: env
                operator: In
                values:
                - three
      containers:
      - image: nginx
        name: deploy01

1. 3 la priorityiteachch 
2. 2 and 3rd same same 30 privority - equel ch ghet
---------------------------------------------------------------------------------------------------------------

Taint and toleration 
 default namespace. 
Ku create deployment saikiran --image nginx: latest --replicas 3 

kubectl taint node i-08cb4fad5c2fd467e high-cpu=yes:NoSchedule
kubectl taint node i-0d255ef313a497963 med-cpu=yes:NoSchedule
kubectl taint node i-0dcf19a32f70443af low-cpu=yes:NoSchedule


apiVersion: apps/v1
kind: Deployment
metadata:
 Labels: 
    app: app1
  name: app1
Spec: 
  replicas: 6
  Selector: 
    matchLabels:
      app: app1
Template: 
  Metadata: 
    Labels: 
      app: app1
  Spec: 
    Containers: 
      - image: nginx: latest 
        name: nginx
    tolerations:
      - key: "high-CPU" 
        operator: "Equal"
        value: "yes"
        effect: "NoSchedule"

apply it

Pending means some taint and toleration occurer  

how to do mountain 
kubectl taint node i-08cb4fad5c2fd467e high-cpu-
kubectl taint node i-0d255ef313a497963 med-CPU- 
kubectl taint node i-0dcf19a32f70443af low-cpu-
--------------------------------------------------------------------------------------------------------------------

Ku create deployment saikiran --image nginx: latest --replicas 3 

ku taint node 1-08cb4fad5c2d467e high-cpu=yes: NoExecute
ku taint node i-0d255ef313a497963 med-cpu=yes:NoExecute
ku taint node i-0dcf19a32f70443af low-cpu=yes:NoExecute 
 Kuu get pods -o wide 
 
apiVersion: apps/v1
kind: Deployment
metadata:
  Labels: 
    app: app2
  name: app2
Spec: 
  replicas: 6
  Selector: 
    matchLabels:
      app: app2
Template: 
  Metadata: 
    Labels: 
      app: app2
  Spec: 
    Containers: 
      - image: nginx: latest 
        name: nginx
    tolerations:
      - key: "med-cpu"
        operator: "Equal"
        value: "yes"
        effect: "NoExecute"
      - key: "lowCPUu" 
        operator: "Equal"
        value: "yes"
        effect: "NoExecute"

apply it 

NoSchedule	❌ New pods CANNOT be scheduled on the node unless they have a matching toleration. Existing pods remain running.

NoExecute	❌ New pods CANNOT be scheduled, and ❌ Existing pods will be REMOVED (evicted) unless they have a matching toleration.

PreferNoSchedule	⚠️ Kubernetes will try to avoid scheduling new pods on this node, but it may still place them
if no other option is available. Existing pods are not affected.
--------------------------------------------------------------------------------------------------------------------------------

 podAntiAffinity:| podAffinity:

---
apiVersion: apps/v1
kind: Deployment
metadata:
  Labels: 
    app: redis
  name: redis
Spec: 
  replicas: 3
  Selector: 
    matchLabels:
      app: redis
  Template: 
    Metadata: 
      Labels: 
        app: redis
    Spec: 
      Affinity: 
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - label selector: 
             Match expressions: 
              - key: app
                operator: In
                values:
                - redis
          topology: "topology.kubernetes.io/zone" 
      containers:
      - image: redis: latest 
        name: redis
---
apiVersion: apps/v1
kind: Deployment
metadata:
Labelss: 
    app: web
  name: web
Spec: 
  replicas: 3
  Selector: 
    matchLabels:
      app: web
  Template: 
    Metadata: 
      Labels: 
        app: web
        perf: high
    Spec: 
      Affinity: 
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - label selector: 
             Match expressions: 
              - key: app
                operator: In
                values:
                - web
          topology: "topology.kubernetes.io/zone" 
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - label selector: 
             Match expressions: 
              - key: app
                operator: In
                values:
                - redis
          topology: "topology.kubernetes.io/zone" 
      containers:
      - image: nginx: latest 
        name: nginx


# ku taint node nodeid high-cpu=yes:N oExecute 
# ku taint nodnode-idid med-cpu=y es:NoExecute  
# ku taintnodee-iaddedlow-cpu =yes:NoExecute  

































