imp cammand :   velero backup create dev-fullbackup --include-namespaces dev --ttl 160h0m0s

                              velero get backup

                               velero get backup this command we have to give on cluster 2 it will automatically sink  

                              kubectl get pod -n velero  -----to see velero install or not

                               kubectl logs (velero pod name)  -n velero  

velero restore create dev-restore  --from-backup dev-fullbackup                    

 

Open image-20250112-050842.png
image-20250112-050842.png
 

Step 1: Create Two EKS Clusters in Different Regions
Cluster 1 (e.g., Cluster-East)
Choose a Region: Decide on a region for the first cluster (e.g., us-east-1).

Create an EKS Cluster:

Use the AWS Management Console, CLI, or Terraform to create an EKS cluster in us-east-1.

Set Up Worker Nodes:

Add worker nodes using managed node groups or self-managed EC2 instances.

Update kubeconfig:

Ensure kubectl is configured for this cluster:



bash
Copy code

aws eks update-kubeconfig --region us-east-1 --name <Cluster-East>

 

Cluster 2 (e.g., Cluster-West)
Choose a Different Region: For example, us-west-2.

Create the Second EKS Cluster:

Follow the same steps as above, but in us-west-2.

Set Up Worker Nodes:

Add worker nodes to this cluster as well.

Update kubeconfig:

Configure kubectl for this cluster:



bash
Copy code

aws eks update-kubeconfig --region us-west-2 --name <Cluster-West>

 

Step 2: Verify Connectivity Between Clusters
Ensure that your kubectl context can switch between the clusters:



bash
Copy code

kubectl config get-contexts

Switch to the desired cluster context:



bash
Copy code

kubectl config use-context <cluster-context-name>

 

 

Step 3: Set Up Velero on Both Clusters
Cluster 1: Install Velero with an S3 bucket configured for backups.

Cluster 2: Install Velero with the same S3 bucket for restores.

 

Step 1: Create the S3 Bucket
Go to the AWS S3 Console:

Open the AWS S3 Console.

Create the S3 Bucket:

Click Create bucket.

Enter a unique bucket name (e.g., velero-backups).

Choose a region (e.g., us-east-1 or a region accessible to both clusters).

Leave Object Ownership set to default (Bucket owner enforced).

Enable Versioning (important for Velero to track changes and maintain backup consistency).

Leave the other options as default and click Create bucket.

Step 2: Create an IAM Role and Policy for S3 Access
Go to the IAM Console:

Open the IAM Console.

Create an IAM Policy for Velero:

Navigate to Policies > Create Policy.

Use the JSON editor and paste the following policy (replace <bucket-name> with your S3 bucket name):



json
 

{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetObject",
        "s3:PutObject",
        "s3:DeleteObject",
        "s3:ListBucket"
      ],
      "Resource": [
        "arn:aws:s3:::<bucket-name>",
        "arn:aws:s3:::<bucket-name>/*"
      ]
    }
  ]
}

 

Click Next and give the policy a name, e.g., VeleroS3AccessPolicy.

Create IAM Roles for Each Cluster:

Go to Roles > Create Role.

Select AWS Service and choose EKS (if you are using EKS) or EC2 (if you are using EC2 worker nodes).

Attach the VeleroS3AccessPolicy to the role.

Name the role (e.g., VeleroCluster1Role for the first cluster and VeleroCluster2Role for the second).

 

To migrate a pod from one Kubernetes cluster (Cluster-1) to another (Cluster-2) using Velero, you need to follow these steps. This includes handling backups, PersistentVolumes (PV), and ConfigMaps to ensure the migration is successful.

Steps to Migrate a Pod from Cluster-1 to Cluster-2
1. Backup Pod and Associated Resources in Cluster-1
Make sure the pod and all its dependencies (like PVs and ConfigMaps) are included in the backup.

Take a Velero backup of the pod's namespace in Cluster-1:



velero backup create backup-kausar --include-namespaces=<namespace>
Replace <namespace> with the namespace of the pod.

Verify the backup:



velero backup get
Ensure the backup shows Phase: Completed.

Check the details of the backup:



velero backup describe backup-kausar --details
Confirm that the pod, ConfigMaps, and PVCs are included in the backup.

2. Prepare Cluster-2 for Migration
Make sure Velero is installed and configured in Cluster-2 to use the same storage backend (e.g., S3 bucket) as Cluster-1.

Install Velero in Cluster-2: Follow Velero installation steps with the same configuration as Cluster-1, ensuring the same storage backend is used.

Switch to Cluster-2 context:



kubectl config use-context <cluster2-context>
3. Restore the Backup in Cluster-2
Restore the pod and its dependencies into Cluster-2.

Restore the backup:



velero restore create --from-backup backup-kausar
Verify the restore:



velero restore get
Look for Phase: Completed.

Check restored resources:

Verify the pod:



kubectl get pods -n <namespace>
Verify ConfigMaps:



kubectl get configmaps -n <namespace>
Verify PVCs:



kubectl get pvc -n <namespace>
4. Handle PersistentVolumes (PV)
Velero backs up PVs as snapshots or references. You need to ensure the storage backend in Cluster-2 matches Cluster-1.

Verify PVs are restored:



kubectl get pv
Update PVs if needed: If the PV is not bound, update the storage class or ensure it points to a valid storage backend in Cluster-2.

5. Test the Restored Pod
Ensure the pod in Cluster-2 is running and functional:

Check pod logs and events:



kubectl logs <pod-name> -n <namespace>
kubectl describe pod <pod-name> -n <namespace>
Test connectivity and endpoints: Verify the application or service running inside the pod works as expected.

6. (Optional) Clean Up Cluster-1
If the pod is no longer needed in Cluster-1, you can delete it after verifying that everything works in Cluster-2:



kubectl delete pod <pod-name> -n <namespace>
Troubleshooting
Missing Resources: If PVCs or ConfigMaps are missing, ensure they are included in the backup and re-run the restore.



velero restore describe <restore-name> --details
Pod Not Running: Check for missing dependencies like Secrets, ConfigMaps, or storage issues.

By following these steps, you can successfully migrate the pod and its associated resources from Cluster-1 to Cluster-2. Let me know if you face any challenges!

 

Namespace
apiVersion: v1
kind: Namespace
metadata:
  name: nginx-namespace

ConfigMap for NGINX Configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-config
  namespace: nginx-namespace
data:
  nginx.conf: |
    server {
        listen 80;
        server_name localhost;
        location / {
            root /usr/share/nginx/html;
            index index.html;
        }
    }

PersistentVolume
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nginx-pv
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /mnt/data/nginx

PersistentVolumeClaim
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nginx-pvc
  namespace: nginx-namespace
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi

Deployment for NGINX
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  namespace: nginx-namespace
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: nginx:latest
          ports:
            - containerPort: 80
          volumeMounts:
            - name: nginx-storage
              mountPath: /usr/share/nginx/html
            - name: nginx-config
              mountPath: /etc/nginx/nginx.conf
              subPath: nginx.conf
      volumes:
        - name: nginx-storage
          persistentVolumeClaim:
            claimName: nginx-pvc
        - name: nginx-config
          configMap:
            name: nginx-config

NodePort Service to Expose NGINX Deployment
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
  namespace: nginx-namespace
spec:
  type: NodePort
  selector:
    app: nginx
  ports:
    - port: 80
      targetPort: 80
      nodePort: 30007  # Ensure this port is open on your nodes

 

 

Namespace YAML - Create the nginx-namespace first.

ConfigMap YAML - Create the nginx-config ConfigMap next.

PersistentVolume YAML - Create the nginx-pv PersistentVolume.

PersistentVolumeClaim YAML - Create the nginx-pvc PersistentVolumeClaim.

Deployment YAML - Create the nginx-deployment.

Service YAML - Finally, create the nginx-service to expose the NGINX application.

 

Add label
