Last week, we faced a critical issue in our log processing pipeline. Our EC2 instance was supposed to upload logs to an S3 bucket, but suddenly, the Jenkins job started failing with:

âŒ 403 Forbidden â€“ Access Denied

At first, everything looked fine:
âœ… IAM role had s3:PutObject permissions.
âœ… S3 bucket existed, and the policy seemed correct.
âœ… Jenkins pipeline was properly configured.
But stillâ€¦ ACCESS DENIED! ğŸ˜¤

ğŸ” The Debugging Process::

ğŸ›  Step 1: Checked IAM Role Permissions â†’ s3:ListBucket was missing!
ğŸ›  Step 2: Reviewed S3 Bucket Policy â†’ The policy didn't explicitly allow access for the EC2 role.
ğŸ›  Step 3: Examined KMS Encryption â†’ The bucket had KMS encryption, but the IAM role lacked kms:Encrypt and kms:Decrypt permissions.
ğŸ›  Step 4: Tested with AWS CLI â†’ Running aws s3 ls s3://my-bucket confirmed Access Denied.

ğŸ’¡ The Fix::

âœ… Updated the IAM Role Policy to include s3:ListBucket.
âœ… Modified the S3 Bucket Policy to explicitly allow EC2 access.
âœ… Added KMS permissions (kms:Encrypt & kms:Decrypt).
âœ… Retested the Jenkins job...SUCCESS! ğŸ‰

ğŸš€ Key Takeaways

ğŸ”¹ Missing s3:ListBucket can break uploads, even if s3:PutObject is allowed.
ğŸ”¹ KMS encryption requires additional permissionsâ€”don't overlook them!
ğŸ”¹ Debugging with AWS CLI (aws s3 ls, aws s3 cp) helps pinpoint access issues fast.
This issue took hours to debug, but the learnings will last a lifetime! ğŸ’¡
ğŸ’¬ Have you ever spent way longer than expected troubleshooting an AWS permissions issue? Letâ€™s discuss in the comments! ğŸš€




Imagine this: 
Itâ€™s a Friday evening, the team is wrapping up a major release, and suddenlyâ€¦ BOOM! A teammate accidentally runs:

git push --force origin main

In an instant, several production commits vanished! The CI/CD pipeline broke, deployments failed, and rollback options looked grim. 
Panic set in. ğŸš¨

How We Recovered (in Real-Time):

ğŸ”¹ Checked git reflog and GitHub commit history â€“ Found the lost commits!
ğŸ”¹ Reverted the damage using git reset --hard and git push --force-with-lease.
ğŸ”¹ Enabled GitHub Branch Protection â€“ No more force pushes to main.
ğŸ”¹ Added Git Pre-Push Hooks â€“ Now, developers get a warning before force pushing.
ğŸ”¹ Mandated PR-based merges â€“ Direct commits to main are now history.

Biggest Lessons Learned:
ğŸš« Never allow force pushes on critical branches.
ğŸ”„ Always have a rollback strategy (git reflog, GitHub backups, audit logs).
ğŸ“Œ Git best practices are a mustâ€”one small mistake can cause massive outages!
This could have been a major production outage, but we caught it in time. Have you faced a GitHub disaster like this? How did you handle it? Letâ€™s discuss! ğŸ‘‡




ğŸš€ How GitHub Solved a Critical Deployment Challenge! ğŸ”¥
Few months back I have faced a major issue in production while deploying a microservices-based application. The problem? A last-minute bug that slipped through testing due to a misconfigured feature flag. ğŸ˜¨
âœ… How GitHub Helped Us Recover Quickly:
1ï¸âƒ£ GitHub Branching Strategy:
We follow a GitFlow approach, ensuring changes go through feature branches.
The bug originated in a hotfix, which needed an urgent patch.
2ï¸âƒ£ GitHub Actions for CI/CD:
A rollback pipeline was already set up using GitHub Actions, so we quickly reverted to the last stable release.
We also improved our pre-merge testing automation to catch such issues earlier.
3ï¸âƒ£ GitHub Security & Code Reviews:
Secret scanning ensured no API keys were exposed.
Code owners feature made sure the right reviewers were tagged for approvals.
4ï¸âƒ£ Infrastructure as Code with Terraform:
All infrastructure changes were version-controlled in GitHub, making rollbacks smooth and predictable.
ğŸ’¡ Key Takeaway:
GitHub is not just a code repositoryâ€”it's a critical tool for collaboration, automation, and security in modern DevOps workflows. A well-structured GitHub workflow can save hours of downtime!
ğŸ’¬ Have you faced similar GitHub challenges? How did you solve them? Letâ€™s discuss in the comments! ğŸ‘‡




ğŸ“Œ Scenario: Your Maven builds are slow because dependencies are downloaded every time.
âœ… Solution: Use a local repository cache and enable parallel builds for faster execution.
ğŸ”— Quick Fix:
1ï¸âƒ£ Configure Maven to use a local repository (settings.xml):

<localRepository>/home/user/.m2/repository</localRepository>

2ï¸âƒ£ Run builds in parallel for multi-module projects:

mvn -T 4 clean install

ğŸ¯ Result: Drastically reduced build time, making your CI/CD pipelines more efficient!
How do you optimize Maven builds in your projects? Drop your thoughts below! ğŸ‘‡
hashtag#Maven hashtag#DevOps hashtag#BuildOptimization hashtag#Performance hashtag#CICD hashtag#Java hashtag#SoftwareEngineering




Ever encountered a port conflict while managing multiple services on a single server? ğŸ¤” Itâ€™s a common scenario where two applications compete for the same port, and the solution often involves changing one serviceâ€™s port. I wanted to share a quick, structured guide!
Hereâ€™s how to change Jenkinsâ€™ default port (8080) to ensure smooth operations:
Step-by-Step Process:

1ï¸âƒ£ Check Jenkins Status:
Run 
systemctl status jenkins 
to ensure the service is active. If not, start it with :
sudo systemctl start jenkins.
2ï¸âƒ£ Identify the Current Port:
Navigate to the Jenkins service file location:

cd /usr/lib/systemd/system 
 
Locate the file:

ls -lrth | grep jenkins.service 

Check the current port:

cat jenkins.service 

3ï¸âƒ£ Update the Port:
Open the service file for editing:

sudo vi /usr/lib/systemd/system/jenkins.service 
Modify --httpPort=8080 to your desired port (e.g., --httpPort=9090).
4ï¸âƒ£ Reload and Restart Jenkins:
Reload system configurations:

sudo systemctl daemon-reload 
Restart the Jenkins service:

sudo systemctl restart jenkins.service 

5ï¸âƒ£ Verify the Changes:
Check if Jenkins is running on the new port:

sudo netstat -nltp | grep java 
Access Jenkins via the updated port (e.g., http://localhost:9090).
6ï¸âƒ£ Update Inbound Rules:
Donâ€™t forget to allow traffic on the new port in your firewall or cloud providerâ€™s security group.

Key Takeaway ::
Changing the default Jenkins port is straightforward and can prevent unnecessary service conflicts. A small tweak can make your systems more reliable and scalable! ğŸ”§
Have you faced similar challenges with port conflicts? Iâ€™d love to hear your experiences or tips in the comments! ğŸ’¬
